# Mixture of Collaborative Experts (MoC)

Uma arquitetura avan√ßada de Mixture of Experts com colabora√ß√£o entre especialistas

---

## üìã Vis√£o Geral

A Mixture of Collaborative Experts (MoC) √© uma evolu√ß√£o do tradicional Mixture of Experts (MoE) que introduz colabora√ß√£o inteligente entre os especialistas selecionados. Ao inv√©s de simplesmente combinar as sa√≠das dos experts de forma independente, a MoC permite que os experts colaborem atrav√©s de cross-attention antes da fus√£o final.

### Diferen√ßas Fundamentais do MoE Tradicional

| Aspecto | MoE Tradicional | MoC (Nossa Implementa√ß√£o) |
|---------|----------------|---------------------------|
| Sele√ß√£o de Experts | Router simples com softmax | Router contextualizado com self-attention |
| Colabora√ß√£o | Nenhuma - experts independentes | Cross-attention entre experts selecionados |
| Auxiliary Loss | Load balancing b√°sico | Diversity + Balance combinados |
| Estabilidade | Problemas de renormaliza√ß√£o | Softmax direto nos logits |

---

## üèóÔ∏è Arquitetura Detalhada

### Componentes Principais

#### 1. Router Contextualization

```python
# Self-attention sobre TODOS os expert outputs
contextualized_experts, _ = self.router_self_attn(
    expert_flat, expert_flat, expert_flat
)
```

**Por que isso √© importante:**
- O router n√£o decide baseado apenas no input token
- Considera as sa√≠das de todos os experts para fazer uma sele√ß√£o mais informada
- Permite routing adaptativo baseado no contexto completo

#### 2. Top-K Selection com Temperature

```python
# Temperature scaling para controle de distribui√ß√£o
routing_logits = routing_logits / self.router_temperature

# Sele√ß√£o est√°vel dos top-k experts
topk_logits, topk_indices = torch.topk(routing_logits, self.top_k, dim=-1)
topk_probs = F.softmax(topk_logits, dim=-1)
```

**Vantagens:**
- Numericamente mais est√°vel que renormaliza√ß√£o manual
- Temperature permite controlar sharpness da distribui√ß√£o
- Evita problemas de divis√£o por zero

#### 3. Collaborative Fusion

```python
# Cross-attention entre experts selecionados
collaborative_outputs, collab_attn = self.collab_cross_attn(
    selected_flat, selected_flat, selected_flat
)

# Refinement adicional
refined_outputs = self.collab_ffn(collaborative_outputs) + collaborative_outputs
```

**O diferencial:**
- Experts selecionados "conversam" entre si via cross-attention
- Refinement FFN adicional para melhorar a colabora√ß√£o
- Residual connections para estabilidade de treino

#### 4. Advanced Auxiliary Loss

```python
def compute_diversity_loss(self, cross_attn_weights, routing_probs):
    # Diversity: maximizar entropia da cross-attention
    attn_entropy = -torch.sum(
        cross_attn_weights * torch.log(cross_attn_weights + 1e-8), dim=-1
    ).mean()
    diversity_loss = -attn_entropy
    
    # Balance: minimizar vari√¢ncia do uso dos experts
    expert_usage = routing_probs.mean(dim=[0, 1])
    balance_loss = torch.var(expert_usage)
    
    return 0.01 * diversity_loss + 0.01 * balance_loss
```

---

## üîÑ Fluxo de Processamento

### Step-by-Step

1. **Input Processing**
   - Recebe tokens: (batch_size, seq_len, d_model)
   - Processa atrav√©s de TODOS os experts em paralelo

2. **Router Contextualization**
   - Self-attention sobre todas as sa√≠das dos experts
   - Gera representa√ß√£o contextualizada para routing

3. **Expert Selection**
   - Aplica temperature scaling nos routing logits
   - Seleciona top-k experts via torch.topk()
   - Calcula probabilidades com softmax est√°vel

4. **Collaborative Fusion**
   - Cross-attention entre experts selecionados
   - Refinement via FFN adicional
   - Residual connections para estabilidade

5. **Final Output**
   - Combina√ß√£o ponderada das sa√≠das colaborativas
   - Proje√ß√£o final + auxiliary loss

### Dimens√µes dos Tensors

```
Input: (B, S, d_model)
Expert Outputs: (B, S, n_experts, d_model)
Contextualized: (B*S, n_experts, d_model)
Top-K Selection: (B, S, top_k, d_model)
Final Output: (B, S, d_model)
```

---

## ‚öôÔ∏è Configura√ß√£o

### Par√¢metros Principais

```python
@dataclass
class LunarisCodexConfig:
    # MoC Specific Parameters
    n_experts: int = 8                    # N√∫mero total de experts
    top_k: int = 2                        # Quantos experts selecionar
    aux_loss_weight: float = 1e-2         # Peso da auxiliary loss
    router_temperature: float = 1.0       # Temperature para routing
```

### Recomenda√ß√µes de Configura√ß√£o

| Par√¢metro | Valor Recomendado | Justificativa |
|-----------|------------------|---------------|
| n_experts | 8-16 | Balance entre capacidade e efici√™ncia |
| top_k | 2-4 | Permite colabora√ß√£o sem overhead excessivo |
| aux_loss_weight | 1e-2 a 1e-3 | Suficiente para regulariza√ß√£o |
| router_temperature | 0.5-2.0 | 1.0 = neutro, <1.0 = mais sharp, >1.0 = mais suave |

---

## üßÆ An√°lise Matem√°tica

### Complexidade Computacional

**Forward Pass:**
- Expert computation: O(B √ó S √ó n_experts √ó d_model¬≤)
- Router self-attention: O(B √ó S √ó n_experts¬≤ √ó d_model)
- Cross-attention: O(B √ó S √ó top_k¬≤ √ó d_model)
- **Total: O(B √ó S √ó n_experts √ó d_model¬≤) (dominante)**

**Compara√ß√£o com MoE tradicional:**
- MoE: O(B √ó S √ó top_k √ó d_model¬≤)
- MoC: O(B √ó S √ó n_experts √ó d_model¬≤) (durante treino)
- **Trade-off: Maior custo computacional por melhor qualidade**

### Auxiliary Loss Breakdown

**Diversity Loss: Encoraja padr√µes diversos na cross-attention**
- Previne collapse dos experts
- Maximiza entropia das attention weights

**Balance Loss: Garante uso equilibrado dos experts**
- Minimiza vari√¢ncia do expert usage
- Evita que alguns experts sejam ignorados

---

## üöÄ Vantagens da MoC

### 1. Melhor Especializa√ß√£o
- Experts colaboram ao inv√©s de competir
- Cada expert pode focar em aspectos espec√≠ficos
- Combina√ß√£o inteligente de conhecimentos

### 2. Routing Contextualizado
- Decis√µes de routing mais informadas
- Considera output de todos os experts
- Adaptativo ao contexto atual

### 3. Estabilidade de Treino
- Auxiliary loss bem balanceada
- Softmax numericamente est√°vel
- Residual connections para gradientes

### 4. Flexibilidade
- Temperature permite tuning fino
- Configur√°vel para diferentes tarefas
- Escal√°vel para mais experts

---

## üîß Implementa√ß√£o

### Integra√ß√£o no Transformer

```python
class Block(nn.Module):
    def __init__(self, config):
        # ... attention layers ...
        
        if config.n_experts is not None and config.n_experts > 0:
            self.feed_forward = CollaborativeExpertsModule(config)
            self.is_moe = True
        else:
            self.feed_forward = FeedForward(config)
            self.is_moe = False
```

### Training Loop Considerations

```python
# Durante o forward pass
logits, loss, past_key_values = model(idx, targets=targets)

if loss is not None:
    total_loss, main_loss, aux_loss = loss
    # total_loss j√° inclui auxiliary loss ponderada
    total_loss.backward()
```

---

## üìä Monitoramento e Debug

### M√©tricas Importantes

**Expert Usage Distribution**
```python
expert_usage = routing_probs.mean(dim=[0, 1])
print(f"Expert usage variance: {torch.var(expert_usage):.4f}")
```

**Auxiliary Loss Components**
```python
print(f"Diversity loss: {diversity_loss:.4f}")
print(f"Balance loss: {balance_loss:.4f}")
```

**Routing Entropy**
```python
routing_entropy = -torch.sum(routing_probs * torch.log(routing_probs + 1e-8), dim=-1).mean()
print(f"Routing entropy: {routing_entropy:.4f}")
```

---

## üéØ Casos de Uso

### Quando Usar MoC

‚úÖ **Ideal para:**
- Tarefas que requerem diferentes tipos de racioc√≠nio
- Modelos grandes onde efici√™ncia √© importante
- Cen√°rios com dados diversificados
- Quando voc√™ quer melhor interpretabilidade

‚ùå **Evitar quando:**
- Modelos muito pequenos (overhead n√£o compensa)
- Tarefas muito espec√≠ficas/homog√™neas
- Recursos computacionais muito limitados
- Prototipagem r√°pida (use FFN padr√£o primeiro)

---

## üî¨ Experimentos e Tuning

### Hyperparameter Sweep Sugerido

```python
# Configura√ß√µes para testar
configs = [
    {"n_experts": 8, "top_k": 2, "router_temperature": 1.0},
    {"n_experts": 8, "top_k": 3, "router_temperature": 0.7},
    {"n_experts": 16, "top_k": 4, "router_temperature": 1.2},
]
```

### Ablation Studies
- **Sem Router Contextualization:** Remove self-attention do router
- **Sem Collaborative Fusion:** Remove cross-attention entre experts
- **Auxiliary Loss Components:** Teste diversity vs balance separadamente

---

## üìö Refer√™ncias e Inspira√ß√µes

### Papers Relacionados
- **Switch Transformer:** Funda√ß√£o do MoE moderno
- **GLaM:** Scaling MoE para modelos gigantes
- **Expert Choice:** Routing improvements

### Diferen√ßas da Nossa Implementa√ß√£o
- Router contextualization com self-attention
- Collaborative fusion via cross-attention
- Auxiliary loss combinada (diversity + balance)
- Integra√ß√£o limpa com arquitetura Llama-style

---

## üêõ Troubleshooting

### Problemas Comuns

**Auxiliary Loss Muito Alta**
- Reduza aux_loss_weight
- Verifique se diversity e balance est√£o balanceados

**Experts N√£o Sendo Usados**
- Aumente router_temperature
- Verifique inicializa√ß√£o dos pesos

**Instabilidade de Treino**
- Reduza learning rate
- Verifique gradient clipping

**Overfitting**
- Aumente dropout
- Reduza n√∫mero de experts ou top_k

---

## üí° Ideias para Extens√µes Futuras

- **Dynamic Top-K:** Ajustar top_k baseado no contexto
- **Hierarchical Experts:** Experts especializados em diferentes n√≠veis
- **Memory-Augmented Routing:** Router com mem√≥ria de decis√µes passadas
- **Multi-Scale Collaboration:** Cross-attention em diferentes escalas

---

**Criado por:** Francisco  
**Data:** Julho 2025  
**Vers√£o:** 1.0

> "A ideia √© simples: ao inv√©s de experts competindo, eles colaboram. E essa colabora√ß√£o acontece atrav√©s de cross-attention, permitindo que cada expert refine sua sa√≠da baseado no que os outros experts est√£o 'pensando'."
